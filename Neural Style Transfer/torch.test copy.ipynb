{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = pathlib.Path(os.getcwd())\n",
    "base_dir = repo_path / \"Neural Style Transfer/\"\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/content images/The-Boy.jpeg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/content images/The-Guardian-2.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/content images/The-Guardian.jpg')],\n",
       " [PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/red-abstract.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/puzzled_women.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/emotional-joshua-miels.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/van-gogh.jpg')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_dir_path = base_dir / \"content images/\"\n",
    "style_dir_path = base_dir / \"style images/\"\n",
    "content_images_path = [\n",
    "    content_dir_path / img \n",
    "    for img in os.listdir(content_dir_path) \n",
    "    if img.split(\".\")[-1] \n",
    "    in [\"jpg\", \"jpeg\", \"png\"]\n",
    "]\n",
    "style_images_path = [\n",
    "    style_dir_path / img\n",
    "    for img in os.listdir(style_dir_path)\n",
    "    if img.split(\".\")[-1] \n",
    "    in [\"jpg\", \"jpeg\", \"png\"]\n",
    "]\n",
    "\n",
    "content_images_path, style_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "def load_image(\n",
    "    image_path: str,\n",
    "    transform=None,\n",
    "    max_size=None,\n",
    "    shape=None,\n",
    "    pad=False\n",
    "):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.Resampling.LANCZOS)\n",
    "\n",
    "    if transform:\n",
    "        image = transform(image)#.unsqueeze(0)\n",
    "\n",
    "    if shape:\n",
    "        image = image.resize(\n",
    "            shape,\n",
    "            Image.LANCZOS\n",
    "        )\n",
    "    if pad:\n",
    "        _, h, w = image.size()\n",
    "        max_hw = max(h, w)\n",
    "        ph = (max_hw - h) / 2\n",
    "        pw = (max_hw - w) / 2\n",
    "        lwp = int(pw if pw % 1 == 0 else pw + 0.5)\n",
    "        rwp = int(pw // 1)\n",
    "        thp = int(ph if ph % 1 == 0 else ph + 0.5)\n",
    "        bhp = int(ph // 1)\n",
    "        image = transforms.functional.pad(\n",
    "            image,\n",
    "            padding=(lwp, thp, rwp, bhp),\n",
    "            fill=0,\n",
    "            padding_mode=\"constant\"\n",
    "        )\n",
    "\n",
    "    return image.numpy()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "# content = load_image( content_images[1], transform, max_size=400, pad=True)\n",
    "# style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n",
    "content_images = torch.Tensor([\n",
    "    load_image(\n",
    "        path_,\n",
    "        transform,\n",
    "        max_size=400,\n",
    "        pad=True\n",
    "    ) for path_ in content_images_path\n",
    "])\n",
    "\n",
    "style_image = load_image(\n",
    "        style_images_path[-1],\n",
    "        transform,\n",
    "        max_size=400,\n",
    "        pad=True\n",
    ")\n",
    "\n",
    "target_images = content_images.clone().requires_grad_(True)\n",
    "\n",
    "content_images = content_images.to(device)\n",
    "style_image = torch.Tensor(style_image).to(device)\n",
    "target_images = target_images.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28']\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vineetmahajan/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torch/_tensor_str.py:103: UserWarning: The operator 'aten::bitwise_and.Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/miniforge3/conda-bld/pytorch-recipe_1664817727684/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='mps:0',\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't optimize a non-leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/torch.test copy.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam([target_images], lr\u001b[39m=\u001b[39;49m\u001b[39m0.003\u001b[39;49m, betas\u001b[39m=\u001b[39;49m[\u001b[39m0.5\u001b[39;49m, \u001b[39m0.999\u001b[39;49m])\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torch/optim/adam.py:90\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid weight_decay value: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(weight_decay))\n\u001b[1;32m     87\u001b[0m defaults \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39mlr, betas\u001b[39m=\u001b[39mbetas, eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m     88\u001b[0m                 weight_decay\u001b[39m=\u001b[39mweight_decay, amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m     89\u001b[0m                 maximize\u001b[39m=\u001b[39mmaximize, foreach\u001b[39m=\u001b[39mforeach, capturable\u001b[39m=\u001b[39mcapturable)\n\u001b[0;32m---> 90\u001b[0m \u001b[39msuper\u001b[39;49m(Adam, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torch/optim/optimizer.py:54\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     51\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: param_groups}]\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m param_group \u001b[39min\u001b[39;00m param_groups:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_param_group(param_group)\n\u001b[1;32m     56\u001b[0m \u001b[39m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# which I don't think exists\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torch/optim/optimizer.py:296\u001b[0m, in \u001b[0;36mOptimizer.add_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39moptimizer can only optimize Tensors, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mbut one of the params is \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtypename(param))\n\u001b[1;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m param\u001b[39m.\u001b[39mis_leaf:\n\u001b[0;32m--> 296\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt optimize a non-leaf Tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m \u001b[39mfor\u001b[39;00m name, default \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m default \u001b[39mis\u001b[39;00m required \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_group:\n",
      "\u001b[0;31mValueError\u001b[0m: can't optimize a non-leaf Tensor"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([target_images], lr=0.003, betas=[0.5, 0.999])\n",
    "# backbone = VGGNet().to(device).eval()\n",
    "\n",
    "# for step in range(1):\n",
    "#     target_features = backbone(target_images)\n",
    "#     content_features = backbone(content_images)\n",
    "#     style_features = backbone(style_image)\n",
    "    \n",
    "    \n",
    "#     n, _, h, w = target_features[-1].shape\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
