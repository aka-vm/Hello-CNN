{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = pathlib.Path(os.getcwd())\n",
    "base_dir = repo_path / \"Neural Style Transfer/\"\n",
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/content images/The-Boy.jpeg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/content images/The-Guardian-2.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/content images/The-Guardian.jpg')],\n",
       " [PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/red-abstract.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/puzzled_women.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/emotional-joshua-miels.jpg'),\n",
       "  PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/style images/van-gogh.jpg')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_dir_path = base_dir / \"content images/\"\n",
    "style_dir_path = base_dir / \"style images/\"\n",
    "content_images_path = [\n",
    "    content_dir_path / img \n",
    "    for img in os.listdir(content_dir_path) \n",
    "    if img.split(\".\")[-1] \n",
    "    in [\"jpg\", \"jpeg\", \"png\"]\n",
    "]\n",
    "style_images_path = [\n",
    "    style_dir_path / img\n",
    "    for img in os.listdir(style_dir_path)\n",
    "    if img.split(\".\")[-1] \n",
    "    in [\"jpg\", \"jpeg\", \"png\"]\n",
    "]\n",
    "\n",
    "content_images_path, style_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/d1l6p8_s53q1w912xq9jh2_80000gn/T/ipykernel_56541/1681988121.py:52: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/miniforge3/conda-bld/pytorch-recipe_1664817727684/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  content_images = torch.Tensor([\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "def load_image(\n",
    "    image_path: str,\n",
    "    transform=None,\n",
    "    max_size=None,\n",
    "    shape=None,\n",
    "    pad=False\n",
    "):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.Resampling.LANCZOS)\n",
    "        \n",
    "    if transform:\n",
    "        image = transform(image)#.unsqueeze(0)\n",
    "\n",
    "    if shape:\n",
    "        image = image.resize(\n",
    "            shape,\n",
    "            Image.LANCZOS\n",
    "        )\n",
    "    if pad:\n",
    "        _, h, w = image.size()\n",
    "        max_hw = max(h, w)\n",
    "        ph = (max_hw - h) / 2\n",
    "        pw = (max_hw - w) / 2\n",
    "        lwp = int(pw if pw % 1 == 0 else pw + 0.5)\n",
    "        rwp = int(pw // 1)\n",
    "        thp = int(ph if ph % 1 == 0 else ph + 0.5)\n",
    "        bhp = int(ph // 1)\n",
    "        image = transforms.functional.pad(\n",
    "            image,\n",
    "            padding=(lwp, thp, rwp, bhp),\n",
    "            fill=0,\n",
    "            padding_mode=\"constant\"\n",
    "        )\n",
    "        \n",
    "\n",
    "    return image.numpy()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "# content = load_image( content_images[1], transform, max_size=400, pad=True)\n",
    "# style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n",
    "content_images = torch.Tensor([\n",
    "    load_image(\n",
    "        path_,\n",
    "        transform,\n",
    "        max_size=400,\n",
    "        pad=True\n",
    "    ) for path_ in content_images_path\n",
    "])\n",
    "\n",
    "style_image = load_image(\n",
    "        style_images_path[-1],\n",
    "        transform,\n",
    "        max_size=400,\n",
    "        pad=True\n",
    ")\n",
    "\n",
    "generated_images = content_images.clone()\n",
    "\n",
    "content_images = content_images.to(device)\n",
    "style_image = torch.Tensor(style_image).to(device)\n",
    "generated_images = generated_images.to(device).requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28']\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "num_images = generated_images.size(0)\n",
    "generated_dir_path = base_dir / \"generated images/\"\n",
    "generated_images_path = [\n",
    "    generated_dir_path / (c_path.name.split(\".\")[0] +\"-\"+ style_images_path[-1].name.split(\".\")[0])\n",
    "    for c_path in content_images_path\n",
    "]\n",
    "[\n",
    "    os.makedirs(path_, exist_ok=True)\n",
    "    for path_ in generated_images_path\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(1)\n",
    "x.to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vineetmahajan/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/vineetmahajan/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/torch.test copy.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m content_features \u001b[39m=\u001b[39m backbone(content_images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m style_features \u001b[39m=\u001b[39m backbone(style_image)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m content_loss \u001b[39m=\u001b[39m style_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(num_images, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat64)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m f_gen, f_con, f_stl \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(generated_features, content_features, style_features):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     content_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((f_gen \u001b[39m-\u001b[39m f_con)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m])\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat64)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([generated_images], lr=0.003, betas=[0.5, 0.999])\n",
    "backbone = VGGNet().to(device).eval()\n",
    "config = {\n",
    "    \"content_weight\": 1,\n",
    "    \"style_weight\": 100,\n",
    "    \"log_step\": 10,\n",
    "    \"total_step\": 1000,\n",
    "    \"save_step\": 100,\n",
    "}\n",
    "# for step in range(config[\"total_step\"]):\n",
    "for step in range(1):\n",
    "    generated_features = backbone(generated_images)\n",
    "    content_features = backbone(content_images)\n",
    "    style_features = backbone(style_image)\n",
    "    \n",
    "    content_loss = style_loss = torch.zeros(num_images, dtype=torch.float64).to(device)\n",
    "    for f_gen, f_con, f_stl in zip(generated_features, content_features, style_features):\n",
    "        content_loss += torch.mean((f_gen - f_con)**2, axis=[1, 2, 3]).to(torch.float64)\n",
    "        \n",
    "        n, c, h, w = f_gen.size()\n",
    "        f_gen = f_gen.view(n, c, h*w)\n",
    "        f_stl = f_stl.view(c, h*w)\n",
    "        \n",
    "        f_gen = torch.bmm(f_gen, f_gen.transpose(1, 2)).to(torch.float64)\n",
    "        f_stl = torch.mm(f_stl, f_stl.t()).to(torch.float64)\n",
    "        \n",
    "        style_loss += torch.mean((f_gen - f_stl)**2, axis=[1, 2])\n",
    "        \n",
    "        \n",
    "    loss = config[\"content_weight\"] * content_loss + config[\"style_weight\"] * style_loss\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    if (step+1) % config[\"log_step\"] == 0:\n",
    "        print(f\"Step [{step+1}/{config['total_step']}]\\n Loss: {loss.item():.4f}\\n\")\n",
    "    if (step+1) % config[\"save_step\"] == 0:\n",
    "        \n",
    "        generated_raw_images = generated_images.clone().detach().cpu()\n",
    "        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "        generated_raw_images = denorm(generated_raw_images).clamp_(0, 1)\n",
    "        [\n",
    "            torchvision.utils.save_image(\n",
    "                img, \n",
    "                path_ / f\"output-{step+1}.png\"\n",
    "            )\n",
    "            for img, path_\n",
    "            in zip(generated_raw_images, generated_images_path)\n",
    "        ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_raw_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/torch.test copy.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(generated_raw_images)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_raw_images' is not defined"
     ]
    }
   ],
   "source": [
    "len(generated_raw_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/generated images/The-Boy-van-gogh'),\n",
       " PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/generated images/The-Guardian-2-van-gogh'),\n",
       " PosixPath('/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/generated images/The-Guardian-van-gogh')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/generated images/The-Boy-van-gogh/output-1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/torch.test copy.ipynb Cell 10\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m denorm \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m-\u001b[39m\u001b[39m2.12\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2.04\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1.80\u001b[39m), (\u001b[39m4.37\u001b[39m, \u001b[39m4.46\u001b[39m, \u001b[39m4.44\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m generated_raw_images \u001b[39m=\u001b[39m denorm(generated_raw_images)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m generated_raw_images \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39msave_image(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         img, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         path_ \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moutput-\u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m img, path_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(generated_raw_images, generated_images_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m generated_raw_images\n",
      "\u001b[1;32m/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/torch.test copy.ipynb Cell 10\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m denorm \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m-\u001b[39m\u001b[39m2.12\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2.04\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1.80\u001b[39m), (\u001b[39m4.37\u001b[39m, \u001b[39m4.46\u001b[39m, \u001b[39m4.44\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m generated_raw_images \u001b[39m=\u001b[39m denorm(generated_raw_images)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m generated_raw_images \u001b[39m=\u001b[39m [\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     torchvision\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49msave_image(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         img, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         path_ \u001b[39m/\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutput-\u001b[39;49m\u001b[39m{\u001b[39;49;00mstep\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m img, path_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(generated_raw_images, generated_images_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural%20Style%20Transfer/torch.test%20copy.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m generated_raw_images\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/torchvision/utils.py:160\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39madd_(\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    159\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 160\u001b[0m im\u001b[39m.\u001b[39;49msave(fp, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-4.12/envs/torch_gpu/lib/python3.9/site-packages/PIL/Image.py:2317\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2316\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mw+b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2319\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2320\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/vineetmahajan/Code/AI/Projects/Hello/Hello-CNN/Neural Style Transfer/generated images/The-Boy-van-gogh/output-1.png'"
     ]
    }
   ],
   "source": [
    "generated_raw_images = generated_images.clone().detach().cpu()\n",
    "denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "generated_raw_images = denorm(generated_raw_images).clamp_(0, 1)\n",
    "generated_raw_images = [\n",
    "    torchvision.utils.save_image(\n",
    "        img, \n",
    "        path_ / f\"output-{step+1}.png\"\n",
    "    )\n",
    "    for img, path_\n",
    "    in zip(generated_raw_images, generated_images_path)\n",
    "]\n",
    "\n",
    "generated_raw_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 625])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_loss += torch.mean((f_gen - f_con)**2, axis=[1, 2, 3])\n",
    "n, c, h, w = f_gen.size()\n",
    "x_gen = f_gen.view(n, c, -1)\n",
    "y_gen = f_stl.view(c, -1)\n",
    "# f_gen.shape\n",
    "x_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 512, 625)\n",
    "\n",
    "# Get the number of matrices in the tensor\n",
    "num_matrices = x.shape[0]\n",
    "\n",
    "# Initialize an empty tensor to store the result\n",
    "result = torch.empty(num_matrices, x.shape[1], x.shape[1])\n",
    "\n",
    "# Perform matrix multiplication for each matrix in the tensor\n",
    "for i in range(num_matrices):\n",
    "    matrix = x[i]  # Get the matrix at index i\n",
    "    result[i] = torch.matmul(matrix, matrix.t())  # Perform matrix multiplication with its transpose\n",
    "\n",
    "# Print the resulting tensor of shape [3, 512, 512]\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 512, 625)\n",
    "x_transposed = x.transpose(1, 2)\n",
    "result = torch.bmm(x, x_transposed)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 625, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 1, 8, 8],\n",
       "         [7, 2, 0, 0],\n",
       "         [2, 2, 0, 9]],\n",
       "\n",
       "        [[7, 6, 2, 9],\n",
       "         [1, 8, 9, 1],\n",
       "         [3, 9, 9, 2]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(0, 10, (2, 3, 4))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 7, 2],\n",
       "         [1, 2, 2],\n",
       "         [8, 0, 0],\n",
       "         [8, 0, 9]],\n",
       "\n",
       "        [[7, 1, 3],\n",
       "         [6, 8, 9],\n",
       "         [2, 9, 9],\n",
       "         [9, 1, 2]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
